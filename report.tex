\documentclass[twocolumn, 11pt,letterpaper]{article}

% --- Packages ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{newtxtext,newtxmath} % Replaces times for better text/math support
\usepackage{microtype} % Improves typography
\usepackage{listings} % For formatted code listings
\lstset{basicstyle=\ttfamily,breaklines=true}

% --- Hyperref Setup ---
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
    pdftitle={Target and Stance Generation using Finetuned Llama},
    pdfauthor={John Doe},
    pdfsubject={Open-Target Stance Detection},
    pdfkeywords={Open-Target Stance Detection, Llama, Finetuning, LoRA, Stance Detection, Target Generation, PEFT},
}

% --- Title Block ---
\title{Target and Stance Generation using Finetuned Llama for Open-Target Stance Detection}
\author{John Doe \\ % Replace with actual name
        Department of Computer Science, University Name \\ % Replace with actual affiliation
        \href{mailto:john.doe@university.edu}{\texttt{john.doe@university.edu}} % Replace with actual email
        }
\date{\today} % Dynamic date for flexibility

% --- Paragraph Formatting ---
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5em}

% --- Document Start ---
\begin{document}

\maketitle

% --- Abstract ---
\begin{abstract}
Stance detection aims to identify the attitude expressed towards a specific target within a text. Traditional methods often assume the target is known a priori. This work addresses the more challenging Open-Target Stance Detection (OTSD) task, where the target must first be identified from the text before determining the stance, without prior knowledge of potential targets during inference. We investigate the adaptation of Large Language Models (LLMs), specifically the Llama 3.1 (8B parameter) architecture, for this task through finetuning. Employing Parameter-Efficient Finetuning (PEFT) with Low-Rank Adaptation (LoRA), we finetuned the base Llama model on a combined dataset derived from established stance detection resources. Evaluation was performed using direct string matching accuracy, comparing the finetuned model against the base model in a zero-shot setting. Our results demonstrate that while the base model struggled with precise target identification (3.89\% direct match accuracy), finetuning significantly improved this capability (17.65\% direct match accuracy). However, the finetuning showed minimal impact on stance detection accuracy (62.10\% base vs. 61.73\% finetuned). This work highlights the potential of finetuning LLMs via LoRA for the OTSD task, particularly for enhancing target identification capabilities within a joint generation framework.
\end{abstract}

% --- Keywords ---
\noindent\textbf{Keywords:} Open-Target Stance Detection, Llama, Finetuning, LoRA, Stance Detection, Target Generation, Parameter-Efficient Finetuning.

% --- Sections ---

\section{Introduction}
\label{sec:introduction}

Stance Detection (SD) is a fundamental task in natural language processing, focusing on determining the viewpoint (e.g., favor, against, neutral) expressed in a text towards a given entity or topic, known as the target \cite{akash2024}. Many real-world applications, such as social media analysis, opinion polling, and argument mining, benefit from accurate stance detection. However, a significant limitation of many existing SD approaches is the requirement that the target of the stance be provided beforehand. This assumption does not hold in many practical scenarios where the target itself needs to be identified from the text.

This leads to the more complex and realistic task of Open-Target Stance Detection (OTSD), where the model must first identify the relevant target(s) within the text and then determine the stance towards that identified target \cite{akash2024}. The OTSD task presents challenges in both target generation (TG) and subsequent stance classification, especially when targets are not explicitly mentioned. While Zero-Shot Stance Detection (ZSSD) methods have emerged to handle targets unseen during training \cite{vast}, they typically still require the target to be provided during inference. Even recent approaches like Target-Stance Extraction (TSE) \cite{tse}, which generate targets, often rely on mapping these generated targets to a predefined list, limiting their truly "open" nature.

Recent advancements in Large Language Models (LLMs) have shown promise in various zero-shot and few-shot learning scenarios. Their ability to understand context and generate text makes them potential candidates for tackling the complexities of OTSD. Initial work by Akash et al. \cite{akash2024} explored the zero-shot capabilities of various LLMs, including Llama, for OTSD, highlighting their potential but also the challenges, particularly with non-explicit targets.

This paper investigates the effectiveness of \textit{finetuning} an LLM for the OTSD task. Specifically, we focus on the Llama 3.1 (8B) architecture \cite{llama3.1} and employ Parameter-Efficient Finetuning (PEFT) using Low-Rank Adaptation (LoRA) \cite{lora}. We train the model on a combined dataset derived from existing stance detection resources (TSE and VAST datasets, as used in \cite{akash2024}) to simultaneously generate the target and predict the stance in a specific format. We evaluate the performance of the finetuned model against its base zero-shot counterpart, primarily using direct string matching accuracy for both generated targets and predicted stances, reflecting the evaluation implemented in our accompanying code. Our contribution lies in demonstrating the impact of task-specific finetuning via LoRA on the Llama model's ability to address the joint target generation and stance detection challenges inherent in OTSD.

\section{Literature Review}
\label{sec:literature}

Research in Stance Detection (SD) has evolved significantly. Early work focused on supervised classification tasks where both the text and the target were provided, often using feature engineering and traditional machine learning models, later transitioning to deep learning approaches like LSTMs and CNNs \cite{vast}.

Recognizing the impracticality of gathering labeled data for every conceivable target, Zero-Shot Stance Detection (ZSSD) emerged. ZSSD aims to predict stances towards targets unseen during training. Various techniques have been explored, including contrastive learning, generative approaches, prompt-based learning with LLMs, and methods incorporating external knowledge \cite{vast}. A prominent line of work, TSE \cite{tse,tts}, generates targets but subsequently maps them to a known target list, facilitating evaluation but deviating from a truly open-target scenario.

The specific challenge of Open-Target Stance Detection (OTSD), where the target is neither provided nor drawn from a predefined inference-time list, was formally introduced and benchmarked by Akash et al. \cite{akash2024}. Their work evaluated the zero-shot performance of several state-of-the-art LLMs (GPT-3.5, GPT-4o, Llama-3, Mistral) on OTSD, establishing baseline capabilities and highlighting the difficulty posed by implicitly mentioned targets. They demonstrated that while LLMs outperform prior methods in target generation quality, significant challenges remain.

Contemporaneously, the capabilities of Large Language Models (LLMs) like Llama \cite{llama3.1}, GPT, and Mistral have rapidly advanced. Their success in diverse generative and instruction-following tasks suggests their potential for the joint generation requirement of OTSD.

However, fully finetuning such large models is computationally expensive. Parameter-Efficient Finetuning (PEFT) methods have been developed to mitigate this. Among these, Low-Rank Adaptation (LoRA) \cite{lora} is a popular technique that introduces trainable low-rank matrices into the existing layers of an LLM, allowing adaptation with significantly fewer trainable parameters compared to full finetuning.

This work builds directly upon the OTSD task defined by Akash et al. \cite{akash2024}. Instead of evaluating zero-shot performance, we investigate the impact of applying PEFT (specifically LoRA) to finetune a Llama 3.1 model explicitly for the combined target generation and stance classification task inherent in OTSD.

\section{Methodology}
\label{sec:methodology}

Our approach to Open-Target Stance Detection (OTSD) involves adapting a pretrained Large Language Model to jointly generate the stance target and classify the stance expressed towards it within a given input text.

\subsection{Problem Definition}
\label{sec:problem_definition}

Let the training dataset be represented as $D = \{(x_i, t_i, y_i)\}_{i=1}^{N}$, where $x_i$ denotes the input text, $t_i$ is the corresponding ground truth target, and $y_i$ is the ground truth stance label (e.g., "FAVOR", "AGAINST", "NONE"). The test dataset is represented similarly as $D' = \{(x'_j, t'_j, y'_j)\}_{j=1}^{M}$, where $x'_j$ represents an input text potentially containing novel targets not seen in $D$.

The core objective in Open-Target Stance Detection (OTSD) is to train a model $M$ that, given an input text $x'$, generates an output sequence $O$ encompassing both a predicted target $\hat{t'}$ and a predicted stance $\hat{y'}$.
Formally:
\[ O = M(x') \]

The generated output sequence $O$ must be parsable to extract the predicted target $\hat{t'}$ and stance $\hat{y'}$. In our implementation, the model is trained using supervised finetuning on $D$ to produce outputs formatted precisely according to a predefined template. Based on the Alpaca instruction format \cite{tts}, the input is structured as:

\begin{lstlisting}
Below is an instruction that describes a task, paired with an
input that provides further context. Write a response that
appropriately completes the request.

### Instruction:
Analyze the stance and identify the target in the following text.

### Input:
[Input Text]

### Response:
Target: [Target], Stance: [Stance]<|end_of_text|>
\end{lstlisting}

During inference, the `[Target]` and `[Stance]` parts are left empty for the model to generate. The goal is to train $M$ such that for unseen inputs $x'$ from the test distribution $D'$, the generated $\hat{t'}$ closely approximates the ground truth target $t'$ and the predicted stance $\hat{y'}$ matches the ground truth stance $y'$.

\subsection{Model Architecture}
\label{sec:model_arch}

The foundation of our approach is the \textbf{Llama 3.1 (8 billion parameter)} model, a highly capable Large Language Model (LLM) developed by Meta \cite{llama3.1}. As a member of the Llama family, Llama 3.1 represents the cutting edge in open foundation models, pretrained on trillions of tokens from a diverse range of publicly available sources. Architecturally, it follows the standard \textbf{transformer-based, decoder-only} paradigm. This means the model processes input sequences and autoregressively generates output tokens one after another, conditioning each new token on the preceding sequence. This inherent generative capability makes it well-suited for tasks requiring text production, such as the joint target and stance generation needed for OTSD. The 8B parameter variant strikes a balance between strong performance and manageable computational requirements for finetuning and inference. For our experiments, we specifically utilized the \texttt{unsloth/Meta-Llama-3.1-8B} version available on Hugging Face, potentially incorporating optimizations provided by the Unsloth library \cite{unsloth}.

While the base Llama 3.1 model possesses extensive world knowledge and language understanding abilities from its pretraining, achieving optimal performance on a specific, nuanced task like OTSD necessitates task-specific adaptation. Fully finetuning all 8 billion parameters, however, demands substantial computational resources (GPU memory and time) often unavailable in typical research settings. To overcome this limitation, we employed \textbf{Parameter-Efficient Finetuning (PEFT)}, a family of techniques designed to adapt large pretrained models by modifying only a small fraction of their parameters.

Specifically, we adopted \textbf{Low-Rank Adaptation (LoRA)} \cite{lora}. LoRA operates on the principle that the change in weights required to adapt a pretrained model to a new task often has a low intrinsic rank. Instead of learning the full change $\Delta W$ for a weight matrix $W$, LoRA learns its low-rank decomposition $\Delta W = BA$, where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$ are significantly smaller matrices (the "LoRA adapters") with rank $r \ll \min(d, k)$. During finetuning, the original pretrained weights $W$ are kept frozen, and only the parameters within the newly introduced matrices $A$ and $B$ are updated via gradient descent. The effective weight matrix during forward passes becomes $W + BA$. This approach dramatically reduces the number of trainable parameters; in our setup, only approximately 0.52\% of the total model parameters were trained.

The specific configuration of LoRA in our experiments, guided by the implementation in the accompanying notebook and common practices for effective adaptation, included:
\begin{itemize}
    \item \textbf{Rank ($r$):} Set to \textbf{16}. This determines the inner dimension of the LoRA matrices $A$ and $B$, controlling the capacity of the adaptation.
    \item \textbf{LoRA Alpha ($\alpha$):} Set to \textbf{16}. This acts as a scaling factor for the LoRA update. The effective update applied to the forward pass is $(\alpha / r) \times BA$. Setting $\alpha = r$ is a common initialization strategy.
    \item \textbf{Targeted Modules:} LoRA adapters were strategically injected into specific layers crucial for language processing within the Llama architecture. These included the query (\texttt{q\_proj}), key (\texttt{k\_proj}), value (\texttt{v\_proj}), and output (\texttt{o\_proj}) projection layers within the multi-head self-attention mechanism, as well as the gate (\texttt{gate\_proj}), up (\texttt{up\_proj}), and down (\texttt{down\_proj}) projection layers within the feed-forward network blocks.
    \item \textbf{LoRA Dropout:} Set to 0, disabling dropout within the LoRA layers for optimized performance as suggested by Unsloth.
    \item \textbf{Bias:} Set to \texttt{"none"}, indicating that bias terms within the LoRA adapters were not trained.
\end{itemize}

To facilitate this finetuning process on resource-constrained hardware (specifically, an NVIDIA T4 GPU with approx. 15GB VRAM as used in the notebook environment), we leveraged the \textbf{Unsloth library} \cite{unsloth}. Unsloth provides optimized implementations for faster training and reduced memory usage. Key techniques enabled by Unsloth in our setup were:
\begin{itemize}
    \item \textbf{4-bit Quantization:} The base Llama 3.1 model weights were loaded in 4-bit precision (\texttt{load\_in\_4bit = True}) using bitsandbytes \cite{bitsandbytes}. This technique significantly compresses the model size, reducing the GPU memory required to load the model.
    \item \textbf{Gradient Checkpointing:} We utilized Unsloth's optimized implementation of gradient checkpointing (\texttt{use\_gradient\_checkpointing = "unsloth"}). This technique avoids storing all intermediate activations during the forward pass, instead recomputing them during the backward pass, drastically reducing memory usage at the cost of increased computation time, enabling training with a sequence length of 2048 tokens.
\end{itemize}

The resulting model, incorporating the frozen Llama 3.1 base weights and the trained LoRA adapters, constitutes our finetuned OTSD model ($M_{\text{finetuned}}$). This architecture is compared against the performance of the base 4-bit quantized Llama 3.1 model ($M_{\text{base}}$) used directly for the task without LoRA adapters or finetuning.

\subsection{Dataset}
\label{sec:dataset}

To train and evaluate our models, we utilized a combined dataset derived from resources established in prior stance detection research \cite{akash2024}. Specifically, we leveraged the datasets associated with the Target-Stance Extraction (TSE) framework \cite{tse} and the VAST dataset \cite{vast}. These datasets cover a diverse range of topics, including politics, social issues, and current events, providing a suitable testbed for the OTSD task.

The combined dataset was preprocessed and split into training and testing sets. The training set, comprising 80\% of the combined data (6,480 examples as per notebook logs), was stored in \texttt{train\_dataset.csv} and used exclusively for finetuning the Llama model with LoRA. The remaining 20\% (1,620 examples) formed the held-out test set, stored in \texttt{test\_dataset.csv}, used for evaluating both the base and finetuned models. Each entry in the datasets contained the input text, the ground truth target, and the ground truth stance (FAVOR, AGAINST, or NONE). The data was formatted into the instruction-following template described in Section \ref{sec:problem_definition} for model input during training and inference.

\subsection{Finetuning Details}
\label{sec:finetuning}

The finetuning process was carried out using the Supervised Finetuning Trainer (\texttt{SFTTrainer}) from the Hugging Face TRL library \cite{trl}, integrated with the Unsloth optimizations. The training objective was standard causal language modeling loss, where the model learns to predict the next token in the sequence, specifically trained to generate the \texttt{Target: ..., Stance: ...} response based on the provided instruction and input text.

Key hyperparameters for the training process were set as follows (based on the notebook implementation):
\begin{itemize}
    \item Maximum Sequence Length: 2048 tokens.
    \item Per Device Train Batch Size: 2.
    \item Gradient Accumulation Steps: 4 (resulting in an effective batch size of 8).
    \item Learning Rate: $2 \times 10^{-4}$.
    \item Optimizer: AdamW 8-bit \cite{adamw8bit}.
    \item Learning Rate Scheduler: Linear decay.
    \item Warmup Steps: 5.
    \item Maximum Training Steps: 60. (Note: This represents a very short training run, chosen for demonstration purposes in the notebook environment. A full run would typically involve training for one or more epochs).
    \item Floating Point Precision: BF16 enabled if supported, otherwise FP16.
    \item Seed: 3407 for reproducibility.
\end{itemize}
Training was performed on a single NVIDIA T4 GPU.

\subsection{Evaluation Metrics}
\label{sec:evaluation}

To assess the performance of both the base ($M_{\text{base}}$) and finetuned ($M_{\text{finetuned}}$) models on the OTSD task, we primarily employed \textbf{Direct Target Matching Accuracy}. This metric calculates the percentage of test samples where the exact string of the predicted target precisely matches the ground truth target string after parsing the model's generated output. Similarly, \textbf{Direct Stance Matching Accuracy} calculates the percentage of samples where the predicted stance label exactly matches the ground truth stance label.

We acknowledge that direct string matching for targets is a very strict metric. It does not account for semantic similarity (e.g., penalizing "climate change action" if the ground truth is "acting on climate change") or minor variations in phrasing. However, it provides a clear, objective measure of the model's ability to replicate the target phrasing learned during training and serves as the primary quantitative evaluation implemented in our accompanying code. Stance accuracy is calculated based on the predicted label matching one of the three categories (FAVOR, AGAINST, NONE).

\section{Experiments}
\label{sec:experiments}

\subsection{Baselines}
\label{sec:baselines}

We compared the performance of two models on the held-out test set:
\begin{itemize}
    \item \textbf{Base Llama Model ($M_{\text{base}}$):} The \texttt{unsloth/Meta-Llama-3.1-8B} model loaded in 4-bit precision. This model was evaluated in a zero-shot setting, meaning it received no specific training on the OTSD task format, although a system prompt was included during inference to guide its output towards the desired \texttt{Target: ..., Stance: ...} format for fair comparison.
    \item \textbf{Finetuned Llama Model ($M_{\text{finetuned}}$):} The same base Llama 3.1 8B model after applying LoRA (r=16, $\alpha$=16) and finetuning for 60 steps on the OTSD training data as described in Section \ref{sec:finetuning}.
\end{itemize}

\subsection{Experimental Setup}
\label{sec:setup}

All experiments were conducted using the infrastructure and libraries specified in the accompanying Jupyter Notebook. This primarily involved Python with the \texttt{torch}, \texttt{transformers}, \texttt{trl}, \texttt{datasets}, and \texttt{unsloth} libraries, executed on a system equipped with an NVIDIA T4 GPU (approx. 15GB VRAM). The evaluation was performed on the 20\% held-out test split (1,620 examples) derived from the combined TSE/VAST dataset. Performance was measured using the Direct Target Matching Accuracy and Direct Stance Matching Accuracy metrics defined in Section \ref{sec:evaluation}.

\section{Results and Discussion}
\label{sec:results}

The performance of the base Llama model ($M_{\text{base}}$) and the finetuned Llama model ($M_{\text{finetuned}}$) on the OTSD test set, evaluated using direct matching accuracy, is presented in Table \ref{tab:results}.

\begin{table}[ht]
\centering
\caption{Direct Matching Accuracy (\%) on the OTSD Test Set (1,620 examples).}
\label{tab:results}
\begin{tabular}{@{}lcc@{}}
\toprule
Model & Target Accuracy (\%) & Stance Accuracy (\%) \\ \midrule
Base Llama Model ($M_{\text{base}}$) & 3.89 & 62.10 \\
Finetuned Llama Model ($M_{\text{finetuned}}$) & 17.65 & 61.73 \\ \bottomrule
\end{tabular}
\end{table}

\paragraph{Target Generation Performance:}
The results clearly indicate that finetuning substantially improved the model's ability to generate the correct target string. The Direct Target Matching Accuracy increased dramatically from 3.89\% for the base model to 17.65\% for the finetuned model. This represents a more than fourfold increase. While a 17.65\% accuracy might seem low in absolute terms, it is crucial to remember the strictness of the direct matching metric. Even minor variations in phrasing (e.g., pluralization, synonyms, slightly different scope) result in a mismatch. The significant improvement suggests that the LoRA finetuning process effectively adapted the model to the specific task of identifying and formulating the target entity as presented in the training data format. The model learned to extract or generate targets that precisely matched the ground truth phrasing much more often than the base model operating in a zero-shot capacity.

\paragraph{Stance Detection Performance:}
In contrast to the target generation results, the Direct Stance Matching Accuracy showed minimal change after finetuning. The base model achieved 62.10\% accuracy, while the finetuned model achieved 61.73\%, a negligible decrease. This suggests that while the finetuning process (even the short 60-step run performed) was highly effective at teaching the model \textit{what} the target was and how to express it, it had little positive impact on improving its ability to classify the stance (FAVOR, AGAINST, NONE) towards that target, at least as measured by direct label matching.

Several factors could contribute to this observation:
\begin{enumerate}
    \item The base Llama model may already possess strong inherent capabilities for classifying stance given a reasonably identified target. Its pretraining likely exposed it to vast amounts of opinionated text. The zero-shot stance accuracy of ~62\% supports this.
    \item The finetuning objective, driven by the causal language modeling loss on the structured \texttt{Target: ..., Stance: ...} output, might have implicitly prioritized learning the target generation component, as it often involves generating more tokens and potentially more complex phrasing than the single stance label.
    \item The short duration of finetuning (60 steps) may not have been sufficient to significantly refine the model's nuanced understanding required for stance classification beyond its pretrained capabilities.
    \item The direct matching metric for stance is straightforward, but improvements in target identification might not directly translate to improved stance classification if the base model already performed reasonably well on the original, perhaps less accurate, targets it implicitly identified.
\end{enumerate}

\paragraph{Overall Impact:}
The experiment demonstrates the efficacy of PEFT, specifically LoRA, for adapting Llama 3.1 to the target generation aspect of the Open-Target Stance Detection task. The substantial gain in target accuracy underscores the value of task-specific finetuning for specialized generative formats. However, the lack of improvement in stance accuracy suggests that either the base model's stance detection ability is already robust, or that further finetuning (longer duration, different hyperparameters, or perhaps different PEFT techniques) might be necessary to significantly enhance stance classification performance within this joint framework. The discrepancy between the large improvement in target accuracy and the minimal change in stance accuracy highlights the distinct challenges posed by the two sub-tasks within OTSD. Furthermore, it reinforces the limitations of using only direct matching for evaluation, as potential improvements in semantic target relevance are not captured by this metric.

\section{Conclusion}
\label{sec:conclusion}

This paper investigated the application of a finetuned Llama-based LLM to the Open-Target Stance Detection (OTSD) task, comparing a base Llama 3.1 8B model against a version finetuned using Low-Rank Adaptation (LoRA) on a combined TSE and VAST dataset. Our findings, based on direct string matching accuracy, demonstrate that while the base LLM possesses some inherent capability for stance detection (~62\% accuracy), it struggles significantly with accurately generating the precise target string in an open setting (less than 4\% accuracy).

Finetuning via LoRA proved highly effective in improving the model's target generation capabilities, increasing the direct target matching accuracy to over 17\%. This significant improvement highlights the potential of PEFT methods for adapting LLMs to the specific generative requirements of identifying and formulating targets in OTSD. However, this enhancement in target generation did not translate into a corresponding improvement in stance detection accuracy, which remained virtually unchanged compared to the base model.

Our results confirm the potential of finetuned LLMs, particularly using efficient methods like LoRA, as a viable approach for addressing the target identification challenge within OTSD. The discrepancy between improvements in target generation and stance detection warrants further investigation. Future work could explore longer finetuning durations, alternative PEFT strategies, incorporating more sophisticated evaluation metrics like semantic similarity or LLM-based evaluation (as suggested in related work \cite{akash2024}), and investigating techniques to explicitly improve the stance classification component alongside target generation during finetuning. Overall, this study provides evidence for the utility of PEFT in adapting LLMs for the complex, generative demands of Open-Target Stance Detection.

% --- References ---
\section*{References}
\begin{thebibliography}{99}

\bibitem{akash2024}
Abu Ubaida Akash, Ahmed Fahmy, and Amine Trabelsi.
\newblock Can Large Language Models Address Open-Target Stance Detection?
\newblock \emph{arXiv preprint arXiv:2409.00222}, 2024.
\newblock \url{https://arxiv.org/abs/2409.00222}.

\bibitem{llama3.1}
Meta AI.
\newblock Llama 3.1: Advancements in Open Foundation Models.
\newblock \emph{Meta AI Technical Report}, July 2024.
\newblock \url{https://ai.meta.com/blog/meta-llama-3-1/}.

\bibitem{lora}
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
\newblock LoRA: Low-Rank Adaptation of Large Language Models.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2022.
\newblock \url{https://openreview.net/forum?id=nZeVKeeFYf9}.

\bibitem{vast}
Emily Allaway and Kathleen McKeown.
\newblock Zero-Shot Stance Detection: A Dataset and Model using Generalized Topic Representations.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 8913--8931, 2020.
\newblock \url{https://doi.org/10.18653/v1/2020.emnlp-main.717}.

\bibitem{tse}
Yingjie Li, Krishna Garg, and Cornelia Caragea.
\newblock A New Direction in Stance Detection: Target-Stance Extraction in the Wild.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 10071--10085, 2023.
\newblock \url{https://doi.org/10.18653/v1/2023.acl-long.561}.

\bibitem{tts}
Yingjie Li, Chenye Zhao, and Cornelia Caragea.
\newblock TTS: A Target-based Teacher-Student Framework for Zero-Shot Stance Detection.
\newblock In \emph{Proceedings of The Web Conference 2023}, pages 1500--1509, 2023.
\newblock \url{https://doi.org/10.1145/3543507.3583249}.

\bibitem{unsloth}
Unsloth AI.
\newblock Unsloth: Efficient Fine-Tuning for Large Language Models.
\newblock \emph{GitHub Repository}, 2024.
\newblock \url{https://github.com/unslothai/unsloth}.

\bibitem{bitsandbytes}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale.
\newblock \emph{arXiv preprint arXiv:2208.07339}, 2022.
\newblock \url{https://arxiv.org/abs/2208.07339}.

\bibitem{trl}
Leandro von Werra, Younes Belkada, Lewis Tunstall, et al.
\newblock TRL: Transformer Reinforcement Learning.
\newblock \emph{GitHub Repository}, 2020.
\newblock \url{https://github.com/huggingface/trl}.

\bibitem{adamw8bit}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock 8-bit Optimizers via Block-wise Quantization.
\newblock \emph{arXiv preprint arXiv:2110.02861}, 2021.
\newblock \url{https://arxiv.org/abs/2110.02861}.

\end{thebibliography}

\end{document}